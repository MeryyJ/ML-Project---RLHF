{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f93952b9",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b924d4af",
   "metadata": {},
   "source": [
    "In this project, we aim to train a language model capable of generating engaging and relevant movie descriptions by leveraging a combination of supervised learning and reinforcement learning. For this purpose, we rely on reviews from the IMDb database, a vast collection of movie critiques written by users and experts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344d7b36",
   "metadata": {},
   "source": [
    "# Data Preparation and Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c566eab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\merie\\OneDrive\\Documents\\GitHub\\ML-Project---RLHF\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import TrainingArguments\n",
    "from trl import RewardTrainer\n",
    "from transformers import GPT2Tokenizer\n",
    "from trl.trainer.reward_trainer import RewardConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "968811c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total avant filtrage: 25000\n",
      "Positives avant: 12500\n",
      "Négatives avant: 12500\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/huggingface/trl/blob/main/examples/notebooks/gpt2-sentiment.ipynb\n",
    "data = load_dataset(\"stanfordnlp/imdb\", split=\"train\")\n",
    "data = data.rename_columns({\"text\": \"review\"})\n",
    "print(f\"Total avant filtrage: {len(data)}\")\n",
    "print(f\"Positives avant: {sum(1 for d in data if d['label'] == 1)}\")\n",
    "print(f\"Négatives avant: {sum(1 for d in data if d['label'] == 0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "084a5a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total après filtrage: 24895\n",
      "Positives après: 12439\n",
      "Négatives après: 12456\n"
     ]
    }
   ],
   "source": [
    "data = data.filter(lambda x: len(x[\"review\"]) > 200, batched=False)\n",
    "print(f\"\\nTotal après filtrage: {len(data)}\")\n",
    "print(f\"Positives après: {sum(1 for d in data if d['label'] == 1)}\")\n",
    "print(f\"Négatives après: {sum(1 for d in data if d['label'] == 0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813f58a1",
   "metadata": {},
   "source": [
    "# Reward Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca7a382f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2\"\n",
    "input_min_text_length = 2\n",
    "input_max_text_length = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9d49c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Initialize the reward model for sequence classification\n",
    "reward_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=1)\n",
    "reward_model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df96a4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_data = data.filter(lambda x: x[\"label\"] == 1)\n",
    "negative_data = data.filter(lambda x: x[\"label\"] == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "548baca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviews positives: 12439\n",
      "Reviews négatives: 12456\n"
     ]
    }
   ],
   "source": [
    "print(f\"Reviews positives: {len(positive_data)}\")\n",
    "print(f\"Reviews négatives: {len(negative_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ede0cd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/huggingface/trl/blob/main/examples/notebooks/gpt2-sentiment.ipynb\n",
    "def sample_length():\n",
    "    return torch.randint(input_min_text_length, input_max_text_length + 1, (1,)).item()\n",
    "\n",
    "# Function to tokenize the review data\n",
    "def tokenize(sample):\n",
    "    max_length = sample_length()\n",
    "    sample[\"input_ids\"] = tokenizer.encode(sample[\"review\"])[:max_length]\n",
    "    sample[\"query\"] = tokenizer.decode(sample[\"input_ids\"])\n",
    "    return sample\n",
    "\n",
    "positive_data = positive_data.map(tokenize, batched=False)\n",
    "negative_data = negative_data.map(tokenize, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9338b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_reviews = positive_data[\"query\"]\n",
    "negative_reviews = negative_data[\"query\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ecd7fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_length = min(len(positive_reviews), len(negative_reviews))\n",
    "chosen = positive_reviews[:min_length]\n",
    "rejected = negative_reviews[:min_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fcd6bf79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward dataset créé avec 12439 paires\n"
     ]
    }
   ],
   "source": [
    "min_length = min(len(positive_reviews), len(negative_reviews))\n",
    "chosen = positive_reviews[:min_length]\n",
    "rejected = negative_reviews[:min_length]\n",
    "\n",
    "reward_data = {\"chosen\": chosen, \"rejected\": rejected}\n",
    "reward_dataset = Dataset.from_dict(reward_data)\n",
    "\n",
    "print(f\"Reward dataset créé avec {len(reward_dataset)} paires\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8819775d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding EOS to train dataset: 100%|██████████| 12439/12439 [00:00<00:00, 28539.83 examples/s]\n",
      "Tokenizing train dataset: 100%|██████████| 12439/12439 [00:02<00:00, 6213.59 examples/s]\n",
      "Filtering train >512 tokens: 100%|██████████| 12439/12439 [00:00<00:00, 83617.46 examples/s]\n",
      "Adding EOS to eval dataset: 100%|██████████| 12439/12439 [00:00<00:00, 33011.37 examples/s]\n",
      "Tokenizing eval dataset: 100%|██████████| 12439/12439 [00:02<00:00, 5674.12 examples/s]\n",
      "Filtering eval >512 tokens: 100%|██████████| 12439/12439 [00:00<00:00, 84534.55 examples/s]\n"
     ]
    }
   ],
   "source": [
    "training_args = RewardConfig(\n",
    "    output_dir=\"./reward_model\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=16,  # Augmenté pour GPU\n",
    "    per_device_eval_batch_size=16,   # Augmenté pour GPU\n",
    "    num_train_epochs=3,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    remove_unused_columns=False,\n",
    "    fp16=True,  # Mixed precision pour GPU\n",
    "    dataloader_num_workers=4,  # Parallélisation\n",
    "    max_length=512,  # Longueur max pour le reward model\n",
    ")\n",
    "\n",
    "reward_trainer = RewardTrainer(\n",
    "    model=reward_model,\n",
    "    train_dataset=reward_dataset,\n",
    "    eval_dataset=reward_dataset,\n",
    "    processing_class=tokenizer,\n",
    "    args=training_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb0c6b67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2334' max='2334' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2334/2334 19:14, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Min Reward</th>\n",
       "      <th>Mean Reward</th>\n",
       "      <th>Max Reward</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Margin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.586500</td>\n",
       "      <td>0.537916</td>\n",
       "      <td>149626.000000</td>\n",
       "      <td>0.237725</td>\n",
       "      <td>3.052803</td>\n",
       "      <td>5.285942</td>\n",
       "      <td>0.723341</td>\n",
       "      <td>0.680680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.499000</td>\n",
       "      <td>0.449922</td>\n",
       "      <td>299252.000000</td>\n",
       "      <td>-0.423991</td>\n",
       "      <td>3.542835</td>\n",
       "      <td>6.939659</td>\n",
       "      <td>0.789662</td>\n",
       "      <td>1.193776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.388100</td>\n",
       "      <td>0.395735</td>\n",
       "      <td>448878.000000</td>\n",
       "      <td>-1.310874</td>\n",
       "      <td>3.751971</td>\n",
       "      <td>7.934824</td>\n",
       "      <td>0.821107</td>\n",
       "      <td>1.666769</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./reward_model\\\\tokenizer_config.json',\n",
       " './reward_model\\\\special_tokens_map.json',\n",
       " './reward_model\\\\vocab.json',\n",
       " './reward_model\\\\merges.txt',\n",
       " './reward_model\\\\added_tokens.json',\n",
       " './reward_model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward_trainer.train()\n",
    "reward_trainer.save_model(\"./reward_model\")\n",
    "tokenizer.save_pretrained(\"./reward_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a1645c",
   "metadata": {},
   "source": [
    "# Optimization with Proximal Policy Optimization (PPO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67700c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "from trl import PPOTrainer, PPOConfig\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Base LM to fine-tune with PPO\n",
    "policy_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Reference model for KL (frozen copy of the SFT model)\n",
    "ref_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "# Reward model: fine-tuned sentiment classifier saved at ./reward_model\n",
    "# (num_labels=1 so it outputs a scalar reward per sequence)\n",
    "reward_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"./reward_model\",\n",
    "    num_labels=1,\n",
    ")\n",
    "\n",
    "# Value model: critic. Same architecture type (scalar regression head).\n",
    "# You can use the same base as the reward model or another checkpoint.\n",
    "value_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=1,\n",
    ")\n",
    "\n",
    "ppo_config = PPOConfig(\n",
    "    exp_name=\"imdb-sentiment-rlhf\",\n",
    "\n",
    "    # Optim / schedule\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=2,      \n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=1,                  \n",
    "    num_ppo_epochs=4,\n",
    "    num_mini_batches=4,                  \n",
    "\n",
    "    # RL-specific\n",
    "    gamma=1.0,\n",
    "    lam=0.95,\n",
    "    cliprange=0.2,\n",
    "    cliprange_value=0.2,\n",
    "    vf_coef=0.1,\n",
    "    kl_coef=0.05,\n",
    "    whiten_rewards=False,\n",
    "\n",
    "    # Generation / stopping\n",
    "    response_length=32,                  \n",
    "    stop_token=None,                     \n",
    "    stop_token_id=None,\n",
    "\n",
    "    # for logging / reproducibility\n",
    "    seed=42,\n",
    "    output_dir=\"./ppo_imdb\",\n",
    "    num_sample_generations=0,\n",
    ")\n",
    "\n",
    "# ppo_config.total_episodes = 1000  \n",
    "\n",
    "def tokenize_for_ppo(sample):\n",
    "    max_length = sample_length()\n",
    "    input_ids = tokenizer.encode(\n",
    "        sample[\"review\"],\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "    )\n",
    "    return {\"input_ids\": input_ids}\n",
    "\n",
    "# Add input_ids column\n",
    "data_with_ids = data.map(tokenize_for_ppo, batched=False)\n",
    "\n",
    "train_input_ids = data_with_ids[\"input_ids\"][:1000]\n",
    "\n",
    "# PPOTrainer expects each element to have at least \"input_ids\"\n",
    "ppo_dataset = [{\"input_ids\": ids} for ids in train_input_ids]\n",
    "\n",
    "# Let PPOTrainer create the default DataCollatorWithPadding using the tokenizer.\n",
    "ppo_trainer = PPOTrainer(\n",
    "    args=ppo_config,\n",
    "    processing_class=tokenizer,\n",
    "    model=policy_model,        # policy\n",
    "    ref_model=ref_model,       # frozen ref policy\n",
    "    reward_model=reward_model, # reward model\n",
    "    value_model=value_model,   # critic\n",
    "    train_dataset=ppo_dataset,\n",
    "    # eval_dataset=...          # optional\n",
    "    # data_collator=...         # leave None to use the default\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca2baaf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting PPO training...\n",
      "===training policy===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 14/125 13:30 < 2:04:54, 0.01 it/s, Epoch 0.10/1.0]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 10\u001b[0m\n\u001b[0;32m      5\u001b[0m value_model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting PPO training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m \u001b[43mppo_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Sauvegarder le modèle PPO\u001b[39;00m\n\u001b[0;32m     13\u001b[0m policy_model\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./ppo_model_final\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\merie\\OneDrive\\Documents\\GitHub\\ML-Project---RLHF\\.venv\\lib\\site-packages\\trl\\trainer\\ppo_trainer.py:643\u001b[0m, in \u001b[0;36mPPOTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    640\u001b[0m mb_return \u001b[38;5;241m=\u001b[39m returns[micro_batch_inds]\n\u001b[0;32m    641\u001b[0m mb_values \u001b[38;5;241m=\u001b[39m values[micro_batch_inds]\n\u001b[1;32m--> 643\u001b[0m output, vpred_temp \u001b[38;5;241m=\u001b[39m \u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmb_query_responses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    644\u001b[0m logits \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mlogits[:, context_length \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m : \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    645\u001b[0m logits \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mtemperature \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-7\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\merie\\OneDrive\\Documents\\GitHub\\ML-Project---RLHF\\.venv\\lib\\site-packages\\trl\\trainer\\utils.py:1059\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, query_responses, pad_token_id)\u001b[0m\n\u001b[0;32m   1057\u001b[0m position_ids \u001b[38;5;241m=\u001b[39m attention_mask\u001b[38;5;241m.\u001b[39mcumsum(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m-\u001b[39m attention_mask\u001b[38;5;241m.\u001b[39mlong()\n\u001b[0;32m   1058\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmasked_fill(query_responses, \u001b[38;5;241m~\u001b[39mattention_mask, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m-> 1059\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1060\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1061\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1062\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1063\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1064\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1065\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\merie\\OneDrive\\Documents\\GitHub\\ML-Project---RLHF\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\merie\\OneDrive\\Documents\\GitHub\\ML-Project---RLHF\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\merie\\OneDrive\\Documents\\GitHub\\ML-Project---RLHF\\.venv\\lib\\site-packages\\accelerate\\utils\\operations.py:819\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 819\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\merie\\OneDrive\\Documents\\GitHub\\ML-Project---RLHF\\.venv\\lib\\site-packages\\accelerate\\utils\\operations.py:807\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    806\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 807\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n",
      "File \u001b[1;32mc:\\Users\\merie\\OneDrive\\Documents\\GitHub\\ML-Project---RLHF\\.venv\\lib\\site-packages\\torch\\amp\\autocast_mode.py:44\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[1;32m---> 44\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\merie\\OneDrive\\Documents\\GitHub\\ML-Project---RLHF\\.venv\\lib\\site-packages\\trl\\trainer\\ppo_trainer.py:126\u001b[0m, in \u001b[0;36mPolicyAndValueWrapper.forward\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 126\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic_backbone(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    127\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_model\u001b[38;5;241m.\u001b[39mscore(output\u001b[38;5;241m.\u001b[39mhidden_states[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), logits\n",
      "File \u001b[1;32mc:\\Users\\merie\\OneDrive\\Documents\\GitHub\\ML-Project---RLHF\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\merie\\OneDrive\\Documents\\GitHub\\ML-Project---RLHF\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\merie\\OneDrive\\Documents\\GitHub\\ML-Project---RLHF\\.venv\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:925\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[1;34m(self, input_ids, past_key_values, cache_position, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m    922\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[0;32m    923\u001b[0m     all_hidden_states \u001b[38;5;241m=\u001b[39m all_hidden_states \u001b[38;5;241m+\u001b[39m (hidden_states,)\n\u001b[1;32m--> 925\u001b[0m outputs \u001b[38;5;241m=\u001b[39m block(\n\u001b[0;32m    926\u001b[0m     hidden_states,\n\u001b[0;32m    927\u001b[0m     past_key_values \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_checkpointing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    928\u001b[0m     cache_position,\n\u001b[0;32m    929\u001b[0m     causal_mask,\n\u001b[0;32m    930\u001b[0m     head_mask[i],\n\u001b[0;32m    931\u001b[0m     encoder_hidden_states,  \u001b[38;5;66;03m# as a positional argument for gradient checkpointing\u001b[39;00m\n\u001b[0;32m    932\u001b[0m     encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_attention_mask,\n\u001b[0;32m    933\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    934\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    935\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    936\u001b[0m )\n\u001b[0;32m    938\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    940\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32mc:\\Users\\merie\\OneDrive\\Documents\\GitHub\\ML-Project---RLHF\\.venv\\lib\\site-packages\\transformers\\modeling_layers.py:94\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     91\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning_once(message)\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m---> 94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\merie\\OneDrive\\Documents\\GitHub\\ML-Project---RLHF\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\merie\\OneDrive\\Documents\\GitHub\\ML-Project---RLHF\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\merie\\OneDrive\\Documents\\GitHub\\ML-Project---RLHF\\.venv\\lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[0;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\merie\\OneDrive\\Documents\\GitHub\\ML-Project---RLHF\\.venv\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:449\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[1;34m(self, hidden_states, past_key_values, cache_position, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, **kwargs)\u001b[0m\n\u001b[0;32m    447\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m    448\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_2(hidden_states)\n\u001b[1;32m--> 449\u001b[0m feed_forward_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;66;03m# residual connection\u001b[39;00m\n\u001b[0;32m    451\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m feed_forward_hidden_states\n",
      "File \u001b[1;32mc:\\Users\\merie\\OneDrive\\Documents\\GitHub\\ML-Project---RLHF\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\merie\\OneDrive\\Documents\\GitHub\\ML-Project---RLHF\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\merie\\OneDrive\\Documents\\GitHub\\ML-Project---RLHF\\.venv\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:376\u001b[0m, in \u001b[0;36mGPT2MLP.forward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    374\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_fc(hidden_states)\n\u001b[0;32m    375\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(hidden_states)\n\u001b[1;32m--> 376\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_proj\u001b[49m(hidden_states)\n\u001b[0;32m    377\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[0;32m    378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[1;32mc:\\Users\\merie\\OneDrive\\Documents\\GitHub\\ML-Project---RLHF\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1927\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1922\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;241m=\u001b[39m OrderedDict()\n\u001b[0;32m   1924\u001b[0m \u001b[38;5;66;03m# It is crucial that the return type is not annotated as `Any`, otherwise type checking\u001b[39;00m\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;66;03m# on `torch.nn.Module` and all its subclasses is largely disabled as a result. See:\u001b[39;00m\n\u001b[0;32m   1926\u001b[0m \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/115074\u001b[39;00m\n\u001b[1;32m-> 1927\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModule\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m   1928\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[0;32m   1929\u001b[0m         _parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "policy_model.to(device)\n",
    "ref_model.to(device)\n",
    "reward_model.to(device)\n",
    "value_model.to(device)\n",
    "\n",
    "print(\"Starting PPO training...\")\n",
    "\n",
    "\n",
    "ppo_trainer.train()\n",
    "\n",
    "# Sauvegarder le modèle PPO\n",
    "policy_model.save_pretrained(\"./ppo_model_final\")\n",
    "tokenize_for_ppo.save_pretrained(\"./ppo_model_final\")\n",
    "print(\"PPO training completed and model saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb203ca",
   "metadata": {},
   "source": [
    "# Text Generation with PPO-Optimized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4237fc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Charger le modèle optimisé\n",
    "generation_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"./ppo_model\",\n",
    "    tokenizer=ppo_tokenizer,\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "# Générer du texte\n",
    "prompts = [\n",
    "    \"This movie is\",\n",
    "    \"The acting was\",\n",
    "    \"I really enjoyed\"\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    result = generation_pipeline(prompt, max_length=50, num_return_sequences=1)\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(f\"Generated: {result[0]['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb3008e",
   "metadata": {},
   "source": [
    "# References and Resources\n",
    "\n",
    "The following resources were used to guide and structure this project. They provided valuable insights into reward modeling, PPO optimization, and the implementation of advanced reinforcement learning techniques for language models:\n",
    "\n",
    "- [GPT-2 Sentiment Analysis Notebook](https://github.com/huggingface/trl/blob/main/examples/notebooks/gpt2-sentiment.ipynb)\n",
    "- [PPO Training Script](https://github.com/huggingface/trl/blob/main/examples/scripts/ppo/ppo.py)\n",
    "- [PPO TLDR Training Script](https://github.com/huggingface/trl/blob/main/examples/scripts/ppo/ppo_tldr.py)\n",
    "- [Reward Modeling Training Script](https://github.com/huggingface/trl/blob/main/examples/scripts/reward_modeling.py)\n",
    "\n",
    "- [CleanRL GitHub Repository](https://github.com/vwxyzjn/cleanrl/tree/master)\n",
    "\n",
    "- [Introduction to PPO and Reinforcement Learning for NLP](https://www.youtube.com/watch?v=hlv79rcHws0&ab_channel=MachineLearningwithPhil)\n",
    "\n",
    "- [Reward Model Training Guide](https://medium.com/towards-generative-ai/reward-model-training-2209d1befb5f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
