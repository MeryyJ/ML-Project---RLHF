{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f93952b9",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b924d4af",
   "metadata": {},
   "source": [
    "In this project, we aim to train a language model capable of generating engaging and relevant movie descriptions by leveraging a combination of supervised learning and reinforcement learning. For this purpose, we rely on reviews from the IMDb database, a vast collection of movie critiques written by users and experts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344d7b36",
   "metadata": {},
   "source": [
    "# Data Preparation and Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c566eab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\merie\\OneDrive\\Documents\\GitHub\\ML-Project---RLHF\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import TrainingArguments\n",
    "from trl import RewardTrainer\n",
    "from transformers import GPT2Tokenizer\n",
    "from trl.trainer.reward_trainer import RewardConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "968811c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total avant filtrage: 25000\n",
      "Positives avant: 12500\n",
      "Négatives avant: 12500\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/huggingface/trl/blob/main/examples/notebooks/gpt2-sentiment.ipynb\n",
    "data = load_dataset(\"stanfordnlp/imdb\", split=\"train\")\n",
    "data = data.rename_columns({\"text\": \"review\"})\n",
    "print(f\"Total avant filtrage: {len(data)}\")\n",
    "print(f\"Positives avant: {sum(1 for d in data if d['label'] == 1)}\")\n",
    "print(f\"Négatives avant: {sum(1 for d in data if d['label'] == 0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "084a5a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total après filtrage: 24895\n",
      "Positives après: 12439\n",
      "Négatives après: 12456\n"
     ]
    }
   ],
   "source": [
    "data = data.filter(lambda x: len(x[\"review\"]) > 200, batched=False)\n",
    "print(f\"\\nTotal après filtrage: {len(data)}\")\n",
    "print(f\"Positives après: {sum(1 for d in data if d['label'] == 1)}\")\n",
    "print(f\"Négatives après: {sum(1 for d in data if d['label'] == 0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813f58a1",
   "metadata": {},
   "source": [
    "# Reward Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca7a382f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2\"\n",
    "input_min_text_length = 2\n",
    "input_max_text_length = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9d49c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Initialize the reward model for sequence classification\n",
    "reward_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=1)\n",
    "reward_model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df96a4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_data = data.filter(lambda x: x[\"label\"] == 1)\n",
    "negative_data = data.filter(lambda x: x[\"label\"] == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "548baca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviews positives: 12439\n",
      "Reviews négatives: 12456\n"
     ]
    }
   ],
   "source": [
    "print(f\"Reviews positives: {len(positive_data)}\")\n",
    "print(f\"Reviews négatives: {len(negative_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ede0cd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/huggingface/trl/blob/main/examples/notebooks/gpt2-sentiment.ipynb\n",
    "def sample_length():\n",
    "    return torch.randint(input_min_text_length, input_max_text_length + 1, (1,)).item()\n",
    "\n",
    "# Function to tokenize the review data\n",
    "def tokenize(sample):\n",
    "    max_length = sample_length()\n",
    "    sample[\"input_ids\"] = tokenizer.encode(sample[\"review\"])[:max_length]\n",
    "    sample[\"query\"] = tokenizer.decode(sample[\"input_ids\"])\n",
    "    return sample\n",
    "\n",
    "positive_data = positive_data.map(tokenize, batched=False)\n",
    "negative_data = negative_data.map(tokenize, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9338b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_reviews = positive_data[\"query\"]\n",
    "negative_reviews = negative_data[\"query\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ecd7fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_length = min(len(positive_reviews), len(negative_reviews))\n",
    "chosen = positive_reviews[:min_length]\n",
    "rejected = negative_reviews[:min_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fcd6bf79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward dataset créé avec 12439 paires\n"
     ]
    }
   ],
   "source": [
    "min_length = min(len(positive_reviews), len(negative_reviews))\n",
    "chosen = positive_reviews[:min_length]\n",
    "rejected = negative_reviews[:min_length]\n",
    "\n",
    "reward_data = {\"chosen\": chosen, \"rejected\": rejected}\n",
    "reward_dataset = Dataset.from_dict(reward_data)\n",
    "\n",
    "print(f\"Reward dataset créé avec {len(reward_dataset)} paires\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8819775d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding EOS to train dataset: 100%|██████████| 12439/12439 [00:00<00:00, 28539.83 examples/s]\n",
      "Tokenizing train dataset: 100%|██████████| 12439/12439 [00:02<00:00, 6213.59 examples/s]\n",
      "Filtering train >512 tokens: 100%|██████████| 12439/12439 [00:00<00:00, 83617.46 examples/s]\n",
      "Adding EOS to eval dataset: 100%|██████████| 12439/12439 [00:00<00:00, 33011.37 examples/s]\n",
      "Tokenizing eval dataset: 100%|██████████| 12439/12439 [00:02<00:00, 5674.12 examples/s]\n",
      "Filtering eval >512 tokens: 100%|██████████| 12439/12439 [00:00<00:00, 84534.55 examples/s]\n"
     ]
    }
   ],
   "source": [
    "training_args = RewardConfig(\n",
    "    output_dir=\"./reward_model\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=16,  # Augmenté pour GPU\n",
    "    per_device_eval_batch_size=16,   # Augmenté pour GPU\n",
    "    num_train_epochs=3,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    remove_unused_columns=False,\n",
    "    fp16=True,  # Mixed precision pour GPU\n",
    "    dataloader_num_workers=4,  # Parallélisation\n",
    "    max_length=512,  # Longueur max pour le reward model\n",
    ")\n",
    "\n",
    "reward_trainer = RewardTrainer(\n",
    "    model=reward_model,\n",
    "    train_dataset=reward_dataset,\n",
    "    eval_dataset=reward_dataset,\n",
    "    processing_class=tokenizer,\n",
    "    args=training_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb0c6b67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2334' max='2334' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2334/2334 19:14, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Min Reward</th>\n",
       "      <th>Mean Reward</th>\n",
       "      <th>Max Reward</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Margin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.586500</td>\n",
       "      <td>0.537916</td>\n",
       "      <td>149626.000000</td>\n",
       "      <td>0.237725</td>\n",
       "      <td>3.052803</td>\n",
       "      <td>5.285942</td>\n",
       "      <td>0.723341</td>\n",
       "      <td>0.680680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.499000</td>\n",
       "      <td>0.449922</td>\n",
       "      <td>299252.000000</td>\n",
       "      <td>-0.423991</td>\n",
       "      <td>3.542835</td>\n",
       "      <td>6.939659</td>\n",
       "      <td>0.789662</td>\n",
       "      <td>1.193776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.388100</td>\n",
       "      <td>0.395735</td>\n",
       "      <td>448878.000000</td>\n",
       "      <td>-1.310874</td>\n",
       "      <td>3.751971</td>\n",
       "      <td>7.934824</td>\n",
       "      <td>0.821107</td>\n",
       "      <td>1.666769</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./reward_model\\\\tokenizer_config.json',\n",
       " './reward_model\\\\special_tokens_map.json',\n",
       " './reward_model\\\\vocab.json',\n",
       " './reward_model\\\\merges.txt',\n",
       " './reward_model\\\\added_tokens.json',\n",
       " './reward_model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward_trainer.train()\n",
    "reward_trainer.save_model(\"./reward_model\")\n",
    "tokenizer.save_pretrained(\"./reward_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a1645c",
   "metadata": {},
   "source": [
    "# Optimization with Proximal Policy Optimization (PPO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "67700c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 23b67c47-5736-4ef9-b2a9-108abf42a329)')' thrown while requesting HEAD https://huggingface.co/gpt2/resolve/main/custom_generate/generate.py\n",
      "Retrying in 1s [Retry 1/5].\n",
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\merie\\OneDrive\\Documents\\GitHub\\ML-Project---RLHF\\.venv\\lib\\site-packages\\trl\\trainer\\ppo_trainer.py:200: UserWarning: This trainer will soon be moved to trl.experimental and is a candidate for removal. If you rely on it and want it to remain, please share your comments here: https://github.com/huggingface/trl/issues/4223. Silence this warning by setting environment variable TRL_EXPERIMENTAL_SILENCE=1.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "from trl import PPOTrainer, PPOConfig\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Base LM to fine-tune with PPO\n",
    "policy_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Reference model for KL (frozen copy of the SFT model)\n",
    "ref_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "# Reward model: fine-tuned sentiment classifier saved at ./reward_model\n",
    "# (num_labels=1 so it outputs a scalar reward per sequence)\n",
    "reward_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"./reward_model\",\n",
    "    num_labels=1,\n",
    ")\n",
    "\n",
    "# Value model: critic. Same architecture type (scalar regression head).\n",
    "# You can use the same base as the reward model or another checkpoint.\n",
    "value_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=1,\n",
    ")\n",
    "\n",
    "ppo_config = PPOConfig(\n",
    "    exp_name=\"imdb-sentiment-rlhf\",\n",
    "\n",
    "    # Optim / schedule\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=2,      \n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=1,                  \n",
    "    num_ppo_epochs=4,\n",
    "    num_mini_batches=4,                  \n",
    "\n",
    "    # RL-specific\n",
    "    gamma=1.0,\n",
    "    lam=0.95,\n",
    "    cliprange=0.2,\n",
    "    cliprange_value=0.2,\n",
    "    vf_coef=0.1,\n",
    "    kl_coef=0.05,\n",
    "    whiten_rewards=False,\n",
    "\n",
    "    # Generation / stopping\n",
    "    response_length=32,                  \n",
    "    stop_token=None,                     \n",
    "    stop_token_id=None,\n",
    "\n",
    "    # for logging / reproducibility\n",
    "    seed=42,\n",
    "    output_dir=\"./ppo_imdb\",\n",
    "    num_sample_generations=0,\n",
    "\n",
    "    no_cuda=False\n",
    ")\n",
    "\n",
    "# ppo_config.total_episodes = 1000  \n",
    "\n",
    "def tokenize_for_ppo(sample):\n",
    "    max_length = sample_length()\n",
    "    input_ids = tokenizer.encode(\n",
    "        sample[\"review\"],\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "    )\n",
    "    return {\"input_ids\": input_ids}\n",
    "\n",
    "# Add input_ids column\n",
    "data_with_ids = data.map(tokenize_for_ppo, batched=False)\n",
    "\n",
    "train_input_ids = data_with_ids[\"input_ids\"][:1000]\n",
    "\n",
    "# PPOTrainer expects each element to have at least \"input_ids\"\n",
    "ppo_dataset = [{\"input_ids\": ids} for ids in train_input_ids]\n",
    "\n",
    "# Let PPOTrainer create the default DataCollatorWithPadding using the tokenizer.\n",
    "ppo_trainer = PPOTrainer(\n",
    "    args=ppo_config,\n",
    "    processing_class=tokenizer,\n",
    "    model=policy_model,        # policy\n",
    "    ref_model=ref_model,       # frozen ref policy\n",
    "    reward_model=reward_model, # reward model\n",
    "    value_model=value_model,   # critic\n",
    "    train_dataset=ppo_dataset,\n",
    "    # eval_dataset=...          # optional\n",
    "    # data_collator=...         # leave None to use the default\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f1bb2a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "NVIDIA GeForce RTX 4050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2baaf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting PPO training...\n",
      "===training policy===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 2:46:49, Epoch 1/1.0]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'save_pretrained'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Sauvegarder le modèle PPO\u001b[39;00m\n\u001b[0;32m     13\u001b[0m policy_model\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./ppo_model_final\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m \u001b[43mtokenize_for_ppo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_pretrained\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./ppo_model_final\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPPO training completed and model saved.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'function' object has no attribute 'save_pretrained'"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "policy_model.to(device)\n",
    "ref_model.to(device)\n",
    "reward_model.to(device)\n",
    "value_model.to(device)\n",
    "\n",
    "print(\"Starting PPO training...\")\n",
    "\n",
    "\n",
    "ppo_trainer.train()\n",
    "\n",
    "# Sauvegarder le modèle PPO\n",
    "policy_model.save_pretrained(\"./ppo_model_final\")\n",
    "tokenize.save_pretrained(\"./ppo_model_final\")\n",
    "print(\"PPO training completed and model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b59c770a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPO training completed and model saved.\n"
     ]
    }
   ],
   "source": [
    "# Sauvegarder le modèle PPO\n",
    "policy_model.save_pretrained(\"./ppo_model_final\")\n",
    "tokenizer.save_pretrained(\"./ppo_model_final\")\n",
    "print(\"PPO training completed and model saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb203ca",
   "metadata": {},
   "source": [
    "# Text Generation with PPO-Optimized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4237fc81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: This movie is\n",
      "Generated: This movie is about a man and his dog who have been separated for more than a year. This is a touching and touching story about a man who believes that all love is real and that every moment of love is precious.\n",
      "\n",
      "A man and his dog are separated as they attempt to reconnect.\n",
      "\n",
      "\"The American Dream\" was written and directed by Eric Kripke, who is a Canadian-American father of two. He is best known for his role as Steve Rogers in the Star Wars: Episode IV A New Hope trilogy and the movie \"The Last Jedi\". He also directed the film \"Blue Sky\" (2011) and \"The Force Awakens\" (2015).\n",
      "\n",
      "For more information on the film, visit www.thestarwars.com\n",
      "\n",
      "Produced by: The Force Awakens\n",
      "\n",
      "Directed by: Eric Kripke\n",
      "\n",
      "Cast: John Boyega (Steve Rogers), Harrison Ford (Captain America), Ben Whishaw (Joker), Robert Downey Jr. (Bella), Daisy Ridley (Harrison Ford), Lupita Nyong'o (Bella), Lupita Nyong'o (Bella), Andressa (Bella), Zoe Saldana (Bella), and Andy Serkis (Bella).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: The acting was\n",
      "Generated: The acting was shot with a.357 Magnum.\n",
      "\n",
      "The shooting occurred after a woman reported being in fear for her life at a local park.\n",
      "\n",
      "The woman who reported the incident was working at the park when she became physically or mentally ill.\n",
      "\n",
      "The female was taken to a local hospital for treatment. The woman was pronounced dead at the scene.\n",
      "\n",
      "Police were able to identify the suspect as 43-year-old Michael J. Johnson.\n",
      "\n",
      "According to police, Johnson was arrested for felony murder, armed illegal handgun possession and robbery.\n",
      "\n",
      "Johnson was arrested at an apartment building and charged with felony murder.\n",
      "\n",
      "The victim was able to escape with minor wounds to her body and was taken to the hospital for treatment.\n",
      "\n",
      "Police are asking anyone with information to call Crime Stoppers at (901) 592-TIPS.\n",
      "\n",
      "Source: http://www.azcentral.com/news/local/african-americans/michael-johnson-pursues-alleged-suicide-after-jail-says-himself-a-stalker-is-a-witness\n",
      "\n",
      "© 2018 Cox Media Group.With your help, we can build a whole new world for women.\n",
      "\n",
      "Prompt: I really enjoyed\n",
      "Generated: I really enjoyed watching this film. I've seen many of the characters and have become more familiar with them. I love what they've been through in their lives and have been inspired to share them with others.\"\n",
      "\n",
      "The film is written and directed by Chris Fadilow, a former United States Marine who now works at the Center for National Security Studies at George Washington University. The film was made for the Academy Award-nominated Documentary Documentary by The Independent Films Project, an independent documentary documentary and television series produced by the Center for National Security Studies. The documentary, which features interviews with the top national security officials, is available from the Center's website at www.ngp.org.The story of the French revolution and the revolution of the twentieth century is told through the eyes of a young French-American woman. Her life and time is shaped by the events of the revolution and the struggles of the French working class.\n",
      "\n",
      "Polls showing that women are at greater risk of becoming victims of rape and of being raped in the workplace, and the rise of the right-wing and anti-immigrant parties in the French working class, have raised the issue of how to protect and defend women.\n",
      "\n",
      "Through the lens of women's rights and feminist history, the\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Charger le modèle optimisé\n",
    "generation_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"./ppo_model_final\",\n",
    "    tokenizer=tokenizer,\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "# Générer du texte\n",
    "prompts = [\n",
    "    \"This movie is\",\n",
    "    \"The acting was\",\n",
    "    \"I really enjoyed\"\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    result = generation_pipeline(prompt, max_length=50, num_return_sequences=1)\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(f\"Generated: {result[0]['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb3008e",
   "metadata": {},
   "source": [
    "# References and Resources\n",
    "\n",
    "The following resources were used to guide and structure this project. They provided valuable insights into reward modeling, PPO optimization, and the implementation of advanced reinforcement learning techniques for language models:\n",
    "\n",
    "- [GPT-2 Sentiment Analysis Notebook](https://github.com/huggingface/trl/blob/main/examples/notebooks/gpt2-sentiment.ipynb)\n",
    "- [PPO Training Script](https://github.com/huggingface/trl/blob/main/examples/scripts/ppo/ppo.py)\n",
    "- [PPO TLDR Training Script](https://github.com/huggingface/trl/blob/main/examples/scripts/ppo/ppo_tldr.py)\n",
    "- [Reward Modeling Training Script](https://github.com/huggingface/trl/blob/main/examples/scripts/reward_modeling.py)\n",
    "\n",
    "- [CleanRL GitHub Repository](https://github.com/vwxyzjn/cleanrl/tree/master)\n",
    "\n",
    "- [Introduction to PPO and Reinforcement Learning for NLP](https://www.youtube.com/watch?v=hlv79rcHws0&ab_channel=MachineLearningwithPhil)\n",
    "\n",
    "- [Reward Model Training Guide](https://medium.com/towards-generative-ai/reward-model-training-2209d1befb5f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
