{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f93952b9",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b924d4af",
   "metadata": {},
   "source": [
    "In this project, we aim to train a language model capable of generating engaging and relevant movie descriptions by leveraging a combination of supervised learning and reinforcement learning. For this purpose, we rely on reviews from the IMDb database, a vast collection of movie critiques written by users and experts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344d7b36",
   "metadata": {},
   "source": [
    "# Data Preparation and Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c566eab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import TrainingArguments\n",
    "from trl import RewardTrainer\n",
    "from transformers import GPT2Tokenizer\n",
    "from trl.trainer.reward_trainer import RewardConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "968811c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a9b25cb07564da9ae4ff7ceb1d09007",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# https://github.com/huggingface/trl/blob/main/examples/notebooks/gpt2-sentiment.ipynb\n",
    "data = load_dataset(\"stanfordnlp/imdb\", split=\"train[:5%]\") # Load the IMDb dataset, selecting 5% of the training split\n",
    "data = data.rename_columns({\"text\": \"review\"})\n",
    "data = data.filter(lambda x: len(x[\"review\"]) > 200, batched=False) # Filter out reviews that are too short "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813f58a1",
   "metadata": {},
   "source": [
    "# Reward Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ca7a382f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2\"\n",
    "input_min_text_length = 2\n",
    "input_max_text_length = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b9d49c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Initialize the reward model for sequence classification\n",
    "reward_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "reward_model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ede0cd37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a548d93d708349208b2068bbcc74cce4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1241 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1168 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: I rented I AM CURIOUS\n",
      "Input IDs: [40, 26399, 314, 3001, 327, 47269, 20958]\n",
      "\n",
      "Query: \"I Am Curious: Yellow\" is\n",
      "Input IDs: [1, 40, 1703, 44269, 25, 12550, 1, 318]\n",
      "\n",
      "Query: If only to\n",
      "Input IDs: [1532, 691, 284]\n",
      "\n",
      "Query: This film\n",
      "Input IDs: [1212, 2646]\n",
      "\n",
      "Query: Oh, brother\n",
      "Input IDs: [5812, 11, 3956]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/huggingface/trl/blob/main/examples/notebooks/gpt2-sentiment.ipynb\n",
    "def sample_length():\n",
    "    return torch.randint(input_min_text_length, input_max_text_length + 1, (1,)).item()\n",
    "\n",
    "# Function to tokenize the review data\n",
    "def tokenize(sample):\n",
    "    max_length = sample_length()\n",
    "    sample[\"input_ids\"] = tokenizer.encode(sample[\"review\"])[:max_length]\n",
    "    sample[\"query\"] = tokenizer.decode(sample[\"input_ids\"])\n",
    "    return sample\n",
    "\n",
    "data = data.map(tokenize, batched=False)\n",
    "\n",
    "for example in data.select(range(5)):\n",
    "    print(\"Query:\", example[\"query\"])\n",
    "    print(\"Input IDs:\", example[\"input_ids\"])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ca30a71b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['chosen', 'rejected'],\n",
      "    num_rows: 620\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Split the reviews into chosen and rejected pairs\n",
    "reviews = data[\"query\"]\n",
    "chosen = reviews[::2] \n",
    "rejected = reviews[1::2] \n",
    "\n",
    "# Ensure both lists are of the same length\n",
    "min_length = min(len(chosen), len(rejected))\n",
    "chosen = chosen[:min_length]\n",
    "rejected = rejected[:min_length]\n",
    "\n",
    "from datasets import Dataset\n",
    "reward_data = {\"chosen\": chosen, \"rejected\": rejected}\n",
    "reward_dataset = Dataset.from_dict(reward_data)\n",
    "\n",
    "print(reward_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "113e2d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom data collator for padding (requested help from ChatGPT due to an error)\n",
    "class CustomRewardDataCollator(DataCollatorWithPadding):\n",
    "    def __call__(self, features):\n",
    "        return super().__call__(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4d899a24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56845085939043de9a6c82e785bf0900",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/620 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d6c7fa3eea949fcb54822ca4c0a7faf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/620 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4a5bc19c8d14a8cab42b0df6889f4fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/620 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c108beaeb0b4f8e8fa5add77cf5770f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/620 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5fe4cafe85045e0980201cbc9e5f1b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/620 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff73888efc354963a3cdeda910db0b48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/620 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='930' max='930' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [930/930 33:57, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.112300</td>\n",
       "      <td>0.949783</td>\n",
       "      <td>0.556452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.928600</td>\n",
       "      <td>0.741319</td>\n",
       "      <td>0.609677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.816100</td>\n",
       "      <td>0.722785</td>\n",
       "      <td>0.625806</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> chosen_text                                           </span>┃<span style=\"font-weight: bold\"> rejected_text             </span>┃<span style=\"font-weight: bold\"> logits           </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩\n",
       "│ I rented I AM CURIOUS                                 │ \"I Am Curious: Yellow\" is │ [0.6708, 0.3292] │\n",
       "├───────────────────────────────────────────────────────┼───────────────────────────┼──────────────────┤\n",
       "│ If only to                                            │ This film                 │ [0.3821, 0.6179] │\n",
       "├───────────────────────────────────────────────────────┼───────────────────────────┼──────────────────┤\n",
       "│ Oh, brother                                           │ I would                   │ [0.3831, 0.6169] │\n",
       "├───────────────────────────────────────────────────────┼───────────────────────────┼──────────────────┤\n",
       "│ Whoever wrote the screenplay for this movie obviously │ When I first saw a        │ [0.3191, 0.6809] │\n",
       "└───────────────────────────────────────────────────────┴───────────────────────────┴──────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mchosen_text                                          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mrejected_text            \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mlogits          \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩\n",
       "│ I rented I AM CURIOUS                                 │ \"I Am Curious: Yellow\" is │ [0.6708, 0.3292] │\n",
       "├───────────────────────────────────────────────────────┼───────────────────────────┼──────────────────┤\n",
       "│ If only to                                            │ This film                 │ [0.3821, 0.6179] │\n",
       "├───────────────────────────────────────────────────────┼───────────────────────────┼──────────────────┤\n",
       "│ Oh, brother                                           │ I would                   │ [0.3831, 0.6169] │\n",
       "├───────────────────────────────────────────────────────┼───────────────────────────┼──────────────────┤\n",
       "│ Whoever wrote the screenplay for this movie obviously │ When I first saw a        │ [0.3191, 0.6809] │\n",
       "└───────────────────────────────────────────────────────┴───────────────────────────┴──────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> chosen_text                                           </span>┃<span style=\"font-weight: bold\"> rejected_text             </span>┃<span style=\"font-weight: bold\"> logits           </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩\n",
       "│ I rented I AM CURIOUS                                 │ \"I Am Curious: Yellow\" is │ [0.5516, 0.4484] │\n",
       "├───────────────────────────────────────────────────────┼───────────────────────────┼──────────────────┤\n",
       "│ If only to                                            │ This film                 │ [0.3883, 0.6117] │\n",
       "├───────────────────────────────────────────────────────┼───────────────────────────┼──────────────────┤\n",
       "│ Oh, brother                                           │ I would                   │ [0.4263, 0.5737] │\n",
       "├───────────────────────────────────────────────────────┼───────────────────────────┼──────────────────┤\n",
       "│ Whoever wrote the screenplay for this movie obviously │ When I first saw a        │ [0.5892, 0.4108] │\n",
       "└───────────────────────────────────────────────────────┴───────────────────────────┴──────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mchosen_text                                          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mrejected_text            \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mlogits          \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩\n",
       "│ I rented I AM CURIOUS                                 │ \"I Am Curious: Yellow\" is │ [0.5516, 0.4484] │\n",
       "├───────────────────────────────────────────────────────┼───────────────────────────┼──────────────────┤\n",
       "│ If only to                                            │ This film                 │ [0.3883, 0.6117] │\n",
       "├───────────────────────────────────────────────────────┼───────────────────────────┼──────────────────┤\n",
       "│ Oh, brother                                           │ I would                   │ [0.4263, 0.5737] │\n",
       "├───────────────────────────────────────────────────────┼───────────────────────────┼──────────────────┤\n",
       "│ Whoever wrote the screenplay for this movie obviously │ When I first saw a        │ [0.5892, 0.4108] │\n",
       "└───────────────────────────────────────────────────────┴───────────────────────────┴──────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> chosen_text                                           </span>┃<span style=\"font-weight: bold\"> rejected_text             </span>┃<span style=\"font-weight: bold\"> logits           </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩\n",
       "│ I rented I AM CURIOUS                                 │ \"I Am Curious: Yellow\" is │ [0.5744, 0.4256] │\n",
       "├───────────────────────────────────────────────────────┼───────────────────────────┼──────────────────┤\n",
       "│ If only to                                            │ This film                 │ [0.6269, 0.3731] │\n",
       "├───────────────────────────────────────────────────────┼───────────────────────────┼──────────────────┤\n",
       "│ Oh, brother                                           │ I would                   │ [0.5131, 0.4869] │\n",
       "├───────────────────────────────────────────────────────┼───────────────────────────┼──────────────────┤\n",
       "│ Whoever wrote the screenplay for this movie obviously │ When I first saw a        │ [0.5097, 0.4903] │\n",
       "└───────────────────────────────────────────────────────┴───────────────────────────┴──────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mchosen_text                                          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mrejected_text            \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mlogits          \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩\n",
       "│ I rented I AM CURIOUS                                 │ \"I Am Curious: Yellow\" is │ [0.5744, 0.4256] │\n",
       "├───────────────────────────────────────────────────────┼───────────────────────────┼──────────────────┤\n",
       "│ If only to                                            │ This film                 │ [0.6269, 0.3731] │\n",
       "├───────────────────────────────────────────────────────┼───────────────────────────┼──────────────────┤\n",
       "│ Oh, brother                                           │ I would                   │ [0.5131, 0.4869] │\n",
       "├───────────────────────────────────────────────────────┼───────────────────────────┼──────────────────┤\n",
       "│ Whoever wrote the screenplay for this movie obviously │ When I first saw a        │ [0.5097, 0.4903] │\n",
       "└───────────────────────────────────────────────────────┴───────────────────────────┴──────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./reward_model\\\\tokenizer_config.json',\n",
       " './reward_model\\\\special_tokens_map.json',\n",
       " './reward_model\\\\vocab.json',\n",
       " './reward_model\\\\merges.txt',\n",
       " './reward_model\\\\added_tokens.json',\n",
       " './reward_model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define custom training arguments (requested help from ChatGPT due to an error)\n",
    "class CustomTrainingArguments(TrainingArguments):\n",
    "    def __init__(self, *args, max_length=512, dataset_num_proc=1, center_rewards_coefficient=1.0, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.max_length = max_length\n",
    "        self.dataset_num_proc = dataset_num_proc\n",
    "        self.center_rewards_coefficient = center_rewards_coefficient\n",
    "\n",
    "# Configure the training arguments\n",
    "training_args = CustomTrainingArguments(\n",
    "    output_dir=\"./reward_model\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=2,  \n",
    "    num_train_epochs=3,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    max_length=512,\n",
    "    dataset_num_proc=1,\n",
    "    center_rewards_coefficient=1.0,\n",
    ")\n",
    "\n",
    "# Initialize the RewardTrainer\n",
    "reward_trainer = RewardTrainer(\n",
    "    model=reward_model,\n",
    "    train_dataset=reward_dataset,\n",
    "    eval_dataset=reward_dataset,\n",
    "    processing_class=tokenizer,\n",
    "    args=training_args,\n",
    "    max_length=None,  \n",
    ")\n",
    "\n",
    "\n",
    "# Train the reward model\n",
    "reward_trainer.train()\n",
    "\n",
    "reward_trainer.save_model(\"./reward_model\")\n",
    "tokenizer.save_pretrained(\"./reward_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fb75c2",
   "metadata": {},
   "source": [
    "The results indicate a steady improvement in model performance across epochs, with the training loss decreasing from 1.1123 to 0.8161 and validation loss reducing from 0.9498 to 0.7228. This demonstrates the model’s ability to generalize better with each epoch. Additionally, the accuracy improved from 55.64% in the first epoch to 62.58% in the final epoch, showcasing the reward model’s increasing effectiveness in evaluating text quality. These findings underscore the potential of combining reward models with PPO for refining language models to generate more coherent and human-aligned outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a1645c",
   "metadata": {},
   "source": [
    "# Optimization with Proximal Policy Optimization (PPO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "f20abb86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from trl import PPOTrainer, PPOConfig\n",
    "\n",
    "ppo_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "ppo_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "ppo_tokenizer.pad_token = ppo_tokenizer.eos_token\n",
    "\n",
    "# Initialize Reference and Value Models\n",
    "ref_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "value_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "ppo_config = PPOConfig(\n",
    "    output_dir=\"./ppo_model\",\n",
    "    learning_rate=1e-5,\n",
    "    batch_size=16,\n",
    "    mini_batch_size=4,\n",
    ")\n",
    "\n",
    "prompts = [\n",
    "    \"What are the best movies of the year?\",\n",
    "    \"Describe a critically acclaimed thriller movie.\",\n",
    "    \"What makes a comedy movie entertaining?\",\n",
    "]\n",
    "\n",
    "def tokenize_prompts(prompts):\n",
    "    tokenized = tokenizer(prompts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    return {\n",
    "        \"input_ids\": tokenized[\"input_ids\"],\n",
    "        \"attention_mask\": tokenized[\"attention_mask\"],\n",
    "    }\n",
    "\n",
    "tokenized_data = tokenize_prompts(prompts)\n",
    "\n",
    "# Define Custom Dataset (requested help from ChatGPT)\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, device):\n",
    "        self.data = data\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data[\"input_ids\"])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: val[idx] for key, val in self.data.items()}\n",
    "    \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_dataset = CustomDataset(tokenized_data, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "5bd6e46a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ppo_model.config.output_hidden_states = True\n",
    "reward_model.config.output_hidden_states = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "f43972b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mery\\AppData\\Local\\Temp\\ipykernel_72300\\2312288454.py:10: FutureWarning: `config` is deprecated and removed starting from version 0.15.0 for `DebugPPOTrainer.__init__`. Use `args` instead.\n",
      "  ppo_trainer = DebugPPOTrainer(\n"
     ]
    }
   ],
   "source": [
    "# Debug PPO Trainer (requested help from ChatGPT)\n",
    "class DebugPPOTrainer(PPOTrainer):\n",
    "    def train(self):\n",
    "        for batch in self.dataloader:\n",
    "            print(\"Training batch:\")\n",
    "            print(batch)\n",
    "            input_ids = batch[\"input_ids\"].to(self.args.device)\n",
    "            print(\"Input IDs shape:\", input_ids.shape)\n",
    "            break  \n",
    "\n",
    "# Initialize PPO Trainer\n",
    "ppo_trainer = DebugPPOTrainer(\n",
    "    config=ppo_config,\n",
    "    model=ppo_model,\n",
    "    ref_model=ref_model,\n",
    "    value_model=value_model,\n",
    "    tokenizer=ppo_tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    reward_model=reward_model,\n",
    ")\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Prepare DataLoader (ChatGTP)\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=ppo_config.batch_size,\n",
    "    collate_fn=lambda x: {\n",
    "        key: torch.cat([item[key] for item in x], dim=0)\n",
    "        for key in x[0]\n",
    "    },\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "ppo_trainer.dataloader = train_dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "4e0a6005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch:\n",
      "{'input_ids': tensor([24564,  4892,   257, 19475, 27023, 32251,  3807,    13, 50256,  2061,\n",
      "         1838,   257, 10997,  3807, 17774,    30, 50256, 50256,  2061,   389,\n",
      "          262,  1266,  6918,   286,   262,   614,    30]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1])}\n",
      "Input IDs shape: torch.Size([27])\n"
     ]
    }
   ],
   "source": [
    "# Run PPO Training\n",
    "ppo_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb203ca",
   "metadata": {},
   "source": [
    "# Text Generation with PPO-Optimized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "cc478ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: Describe a heartwarming drama movie.\n",
      "\n",
      "Daniels played a friend of an old friend of his who is now dead.\n",
      "\n",
      "Ride the Carousel\n",
      "\n",
      "When an ex-con returns to her fishing village, she blasts off on a date with a big brat.\n",
      "\n",
      "Sunderland Tilapia\n",
      "\n",
      "After studyingancy in Rome for two years, a single U.S. tourist tries heartbreak by staying up all night watching the latest movies.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generation_pipeline = pipeline(\"text-generation\", model=ppo_model, tokenizer=ppo_tokenizer)\n",
    "result = generation_pipeline(\"Describe a heartwarming drama movie.\", max_length=100)\n",
    "print(\"Generated Text:\", result[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8eee0c4",
   "metadata": {},
   "source": [
    "The text generation process successfully demonstrates that the PPO-optimized model can respond to prompts and generate content. However, the quality of the output lacks coherence and does not fully align with the context of the prompt. This may be attributed to the absence of a properly fine-tuned Hugging Face reward model and incomplete optimization steps. Further refinement, including better data preprocessing and reinforcement training, is required to enhance the clarity and relevance of the generated text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb3008e",
   "metadata": {},
   "source": [
    "# References and Resources\n",
    "\n",
    "The following resources were used to guide and structure this project. They provided valuable insights into reward modeling, PPO optimization, and the implementation of advanced reinforcement learning techniques for language models:\n",
    "\n",
    "- [GPT-2 Sentiment Analysis Notebook](https://github.com/huggingface/trl/blob/main/examples/notebooks/gpt2-sentiment.ipynb)\n",
    "- [PPO Training Script](https://github.com/huggingface/trl/blob/main/examples/scripts/ppo/ppo.py)\n",
    "- [PPO TLDR Training Script](https://github.com/huggingface/trl/blob/main/examples/scripts/ppo/ppo_tldr.py)\n",
    "- [Reward Modeling Training Script](https://github.com/huggingface/trl/blob/main/examples/scripts/reward_modeling.py)\n",
    "\n",
    "- [CleanRL GitHub Repository](https://github.com/vwxyzjn/cleanrl/tree/master)\n",
    "\n",
    "- [Introduction to PPO and Reinforcement Learning for NLP](https://www.youtube.com/watch?v=hlv79rcHws0&ab_channel=MachineLearningwithPhil)\n",
    "\n",
    "- [Reward Model Training Guide](https://medium.com/towards-generative-ai/reward-model-training-2209d1befb5f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
